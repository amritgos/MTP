{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrape_Sentiment_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI5Px5CjNn6G",
        "outputId": "dc3d6d96-6922-4bc4-cdce-1ba1a7d9793e"
      },
      "source": [
        "!pip install twint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twint\n",
            "  Downloading twint-2.1.20.tar.gz (31 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 12.6 MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint) (4.6.3)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 46.5 MB/s \n",
            "\u001b[?25hCollecting elasticsearch\n",
            "  Downloading elasticsearch-7.15.2-py2.py3-none-any.whl (379 kB)\n",
            "\u001b[K     |████████████████████████████████| 379 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from twint) (1.1.5)\n",
            "Collecting aiohttp_socks\n",
            "  Downloading aiohttp_socks-0.7.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from twint) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Collecting googletransx\n",
            "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.21)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.10.0.2)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (2.0.7)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (21.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\n",
            "Collecting python-socks[asyncio]<3.0.0,>=2.0.0\n",
            "  Downloading python_socks-2.0.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint) (1.52)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->googletransx->twint) (3.0.4)\n",
            "Building wheels for collected packages: twint, fake-useragent, googletransx\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.20-py3-none-any.whl size=33930 sha256=2449b15440ce3d4f5080cb4043175741665496437e0816c8de4a4c2588adb650\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/fc/77/99887a36b5c265a87516158858697d1a0b8f32c4d4dbddbb24\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=e31316e8c7d6e65883b46df72b7c3ce6af7be045c111a4ff730607761b791d99\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=714356470833704ee489c892086b9bcc5007a374d79a5da7d9bd63156478e5f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\n",
            "Successfully built twint fake-useragent googletransx\n",
            "Installing collected packages: multidict, frozenlist, yarl, python-socks, asynctest, async-timeout, aiosignal, pycares, aiohttp, schedule, googletransx, fake-useragent, elasticsearch, cchardet, aiohttp-socks, aiodns, twint\n",
            "Successfully installed aiodns-3.0.0 aiohttp-3.8.1 aiohttp-socks-0.7.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 cchardet-2.1.7 elasticsearch-7.15.2 fake-useragent-0.1.11 frozenlist-1.2.0 googletransx-2.4.2 multidict-5.2.0 pycares-4.1.2 python-socks-2.0.0 schedule-1.1.0 twint-2.1.20 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVdjlHABNylg",
        "outputId": "66cfc2a7-f987-40cc-f3af-201ae76949d4"
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=8d49547645d157d9867b89b64431a4b8d10bb71614fc0ed980e887ded8e529f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.8 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0wqCcDEN4dY",
        "outputId": "2c74dd8e-d118-4b6e-a9e8-77c23e23ef5f"
      },
      "source": [
        "!pip install dateparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dateparser\n",
            "  Downloading dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 184 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 245 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 276 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 286 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 288 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.7/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NCPQAWL881Y"
      },
      "source": [
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "# from News_Scraper import GoogleNews\n",
        "import twint\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nBVWQJdNlbj"
      },
      "source": [
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib\n",
        "from dateparser import parse as parse_date\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "class GoogleNews:\n",
        "    def __init__(self, lang = 'en', country = 'US'):\n",
        "        self.lang = lang.lower()\n",
        "        self.country = country.upper()\n",
        "        self.BASE_URL = 'https://news.google.com/rss'\n",
        "\n",
        "    def __top_news_parser(self, text):\n",
        "        \"\"\"Return subarticles from the main and topic feeds\"\"\"\n",
        "        try:\n",
        "            bs4_html = BeautifulSoup(text, \"html.parser\")\n",
        "            # find all li tags\n",
        "            lis = bs4_html.find_all('li')\n",
        "            sub_articles = []\n",
        "            for li in lis:\n",
        "                try:\n",
        "                    sub_articles.append({\"url\": li.a['href'],\n",
        "                                         \"title\": li.a.text,\n",
        "                                         \"publisher\": li.font.text})\n",
        "                except:\n",
        "                    pass\n",
        "            return sub_articles\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def __ceid(self):\n",
        "        \"\"\"Compile correct country-lang parameters for Google News RSS URL\"\"\"\n",
        "        return '?ceid={}:{}&hl={}&gl={}'.format(self.country,self.lang,self.lang,self.country)\n",
        "\n",
        "    def __add_sub_articles(self, entries):\n",
        "        for i, val in enumerate(entries):\n",
        "            if 'summary' in entries[i].keys():\n",
        "                entries[i]['sub_articles'] = self.__top_news_parser(entries[i]['summary'])\n",
        "            else:\n",
        "                entries[i]['sub_articles'] = None\n",
        "        return entries\n",
        "\n",
        "    def __scaping_bee_request(self, api_key, url):\n",
        "        response = requests.get(\n",
        "            url=\"https://app.scrapingbee.com/api/v1/\",\n",
        "            params={\n",
        "                \"api_key\": api_key,\n",
        "                \"url\": url,\n",
        "                \"render_js\": \"false\"\n",
        "            }\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            return response\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(\"ScrapingBee status_code: \"  + str(response.status_code) + \" \" + response.text)\n",
        "\n",
        "    def __parse_feed(self, feed_url, proxies=None, scraping_bee = None):\n",
        "\n",
        "        if scraping_bee and proxies:\n",
        "            raise Exception(\"Pick either ScrapingBee or proxies. Not both!\")\n",
        "\n",
        "        if proxies:\n",
        "            r = requests.get(feed_url, proxies = proxies)\n",
        "        else:\n",
        "            r = requests.get(feed_url)\n",
        "\n",
        "        if scraping_bee:\n",
        "            r = self.__scaping_bee_request(url = feed_url, api_key = scraping_bee)\n",
        "        else:\n",
        "            r = requests.get(feed_url)\n",
        "\n",
        "\n",
        "        if 'https://news.google.com/rss/unsupported' in r.url:\n",
        "            raise Exception('This feed is not available')\n",
        "\n",
        "        d = feedparser.parse(r.text)\n",
        "\n",
        "        if not scraping_bee and not proxies and len(d['entries']) == 0:\n",
        "            d = feedparser.parse(feed_url)\n",
        "\n",
        "        return dict((k, d[k]) for k in ('feed', 'entries'))\n",
        "\n",
        "    def __search_helper(self, query):\n",
        "        return urllib.parse.quote_plus(query)\n",
        "\n",
        "    def __from_to_helper(self, validate=None):\n",
        "        try:\n",
        "            validate = parse_date(validate).strftime('%Y-%m-%d')\n",
        "            return str(validate)\n",
        "        except:\n",
        "            raise Exception('Could not parse your date')\n",
        "\n",
        "\n",
        "\n",
        "    def top_news(self, proxies=None, scraping_bee = None):\n",
        "        \"\"\"Return a list of all articles from the main page of Google News\n",
        "        given a country and a language\"\"\"\n",
        "        d = self.__parse_feed(self.BASE_URL + self.__ceid(), proxies=proxies, scraping_bee=scraping_bee)\n",
        "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
        "        return d\n",
        "\n",
        "    def topic_headlines(self, topic: str, proxies=None, scraping_bee=None):\n",
        "        \"\"\"Return a list of all articles from the topic page of Google News\n",
        "        given a country and a language\"\"\"\n",
        "        #topic = topic.upper()\n",
        "        if topic.upper() in ['WORLD', 'NATION', 'BUSINESS', 'TECHNOLOGY', 'ENTERTAINMENT', 'SCIENCE', 'SPORTS', 'HEALTH']:\n",
        "            d = self.__parse_feed(self.BASE_URL + '/headlines/section/topic/{}'.format(topic.upper()) + self.__ceid(), proxies = proxies, scraping_bee=scraping_bee)\n",
        "\n",
        "        else:\n",
        "            d = self.__parse_feed(self.BASE_URL + '/topics/{}'.format(topic) + self.__ceid(), proxies = proxies, scraping_bee=scraping_bee)\n",
        "\n",
        "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
        "        if len(d['entries']) > 0:\n",
        "            return d\n",
        "        else:\n",
        "            raise Exception('unsupported topic')\n",
        "\n",
        "    def geo_headlines(self, geo: str, proxies=None, scraping_bee=None):\n",
        "        \"\"\"Return a list of all articles about a specific geolocation\n",
        "        given a country and a language\"\"\"\n",
        "        d = self.__parse_feed(self.BASE_URL + '/headlines/section/geo/{}'.format(geo) + self.__ceid(), proxies = proxies, scraping_bee=scraping_bee)\n",
        "\n",
        "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
        "        return d\n",
        "\n",
        "    def search(self, query: str, helper = True, when = None, from_ = None, to_ = None, proxies=None, scraping_bee=None):\n",
        "        \"\"\"\n",
        "        Return a list of all articles given a full-text search parameter,\n",
        "        a country and a language\n",
        "        :param bool helper: When True helps with URL quoting\n",
        "        :param str when: Sets a time range for the artiles that can be found\n",
        "        \"\"\"\n",
        "\n",
        "        if when:\n",
        "            query += ' when:' + when\n",
        "\n",
        "        if from_ and not when:\n",
        "            # from_ = self.__from_to_helper(validate=from_)\n",
        "            query += ' after:' + from_\n",
        "\n",
        "        if to_ and not when:\n",
        "            # to_ = self.__from_to_helper(validate=to_)\n",
        "            query += ' before:' + to_\n",
        "\n",
        "        if helper == True:\n",
        "            query = self.__search_helper(query)\n",
        "\n",
        "        search_ceid = self.__ceid()\n",
        "        search_ceid = search_ceid.replace('?', '&')\n",
        "\n",
        "        d = self.__parse_feed(self.BASE_URL + '/search?q={}'.format(query) + search_ceid, proxies = proxies, scraping_bee=scraping_bee)\n",
        "\n",
        "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
        "        return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGby3Ht58syl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2685c446-b531-4030-a6b1-b46a19f89d06"
      },
      "source": [
        "table=pd.read_html('https://en.wikipedia.org/wiki/NIFTY_50')\n",
        "df_list = table[1]\n",
        "print(df_list)\n",
        "symbols = df_list['Symbol']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Company Name      Symbol                      Sector\n",
            "0                       Adani Ports  ADANIPORTS              Infrastructure\n",
            "1                      Asian Paints  ASIANPAINT              Consumer Goods\n",
            "2                         Axis Bank    AXISBANK                     Banking\n",
            "3                        Bajaj Auto  BAJAJ-AUTO                  Automobile\n",
            "4                     Bajaj Finance  BAJFINANCE          Financial Services\n",
            "5                     Bajaj Finserv  BAJAJFINSV          Financial Services\n",
            "6                     Bharti Airtel  BHARTIARTL           Telecommunication\n",
            "7                  Bharat Petroleum        BPCL          Energy - Oil & Gas\n",
            "8              Britannia Industries   BRITANNIA              Consumer Goods\n",
            "9                             Cipla       CIPLA             Pharmaceuticals\n",
            "10                       Coal India   COALINDIA                      Metals\n",
            "11              Divi's Laboratories    DIVISLAB             Pharmaceuticals\n",
            "12         Dr. Reddy's Laboratories     DRREDDY             Pharmaceuticals\n",
            "13                    Eicher Motors   EICHERMOT                  Automobile\n",
            "14                Grasim Industries      GRASIM                      Cement\n",
            "15                 HCL Technologies     HCLTECH      Information Technology\n",
            "16                             HDFC        HDFC          Financial Services\n",
            "17                        HDFC Bank    HDFCBANK                     Banking\n",
            "18                        HDFC Life    HDFCLIFE          Financial Services\n",
            "19                    Hero MotoCorp  HEROMOTOCO                  Automobile\n",
            "20              Hindalco Industries    HINDALCO                      Metals\n",
            "21               Hindustan Unilever  HINDUNILVR              Consumer Goods\n",
            "22                       ICICI Bank   ICICIBANK                     Banking\n",
            "23                    IndusInd Bank  INDUSINDBK                     Banking\n",
            "24                          Infosys        INFY      Information Technology\n",
            "25           Indian Oil Corporation         IOC          Energy - Oil & Gas\n",
            "26                      ITC Limited         ITC              Consumer Goods\n",
            "27                        JSW Steel    JSWSTEEL                      Metals\n",
            "28              Kotak Mahindra Bank   KOTAKBANK                     Banking\n",
            "29                  Larsen & Toubro          LT                Construction\n",
            "30              Mahindra & Mahindra         M&M                  Automobile\n",
            "31                    Maruti Suzuki      MARUTI                  Automobile\n",
            "32                     Nestlé India   NESTLEIND              Consumer Goods\n",
            "33                             NTPC        NTPC              Energy - Power\n",
            "34  Oil and Natural Gas Corporation        ONGC          Energy - Oil & Gas\n",
            "35  Power Grid Corporation of India   POWERGRID              Energy - Power\n",
            "36              Reliance Industries    RELIANCE          Energy - Oil & Gas\n",
            "37              State Bank of India        SBIN                     Banking\n",
            "38       SBI Life Insurance Company     SBILIFE          Financial Services\n",
            "39                    Shree Cements    SHREECEM                      Cement\n",
            "40               Sun Pharmaceutical   SUNPHARMA             Pharmaceuticals\n",
            "41                      Tata Motors  TATAMOTORS                  Automobile\n",
            "42                       Tata Steel   TATASTEEL                      Metals\n",
            "43        Tata Consultancy Services         TCS      Information Technology\n",
            "44           Tata Consumer Products  TATACONSUM              Consumer Goods\n",
            "45                    Tech Mahindra       TECHM      Information Technology\n",
            "46                    Titan Company       TITAN           Consumer Durables\n",
            "47                 UltraTech Cement  ULTRACEMCO                      Cement\n",
            "48        United Phosphorus Limited         UPL                   Chemicals\n",
            "49                            Wipro       WIPRO  Information Technology[14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MI7APHekXiKr",
        "outputId": "fe844dcb-5edc-4457-f614-704ece73f0af"
      },
      "source": [
        "list_stocks = ['RELIANCE', 'TCS', 'HDFC', 'HINDUNILVR', 'INFY']\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for tic in list_stocks:\n",
        "  val = df_list.loc[df_list.Symbol == tic]\n",
        "  df = df.append(val,ignore_index = True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Sector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reliance Industries</td>\n",
              "      <td>RELIANCE</td>\n",
              "      <td>Energy - Oil &amp; Gas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tata Consultancy Services</td>\n",
              "      <td>TCS</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HDFC</td>\n",
              "      <td>HDFC</td>\n",
              "      <td>Financial Services</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hindustan Unilever</td>\n",
              "      <td>HINDUNILVR</td>\n",
              "      <td>Consumer Goods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Infosys</td>\n",
              "      <td>INFY</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Company Name      Symbol                  Sector\n",
              "0        Reliance Industries    RELIANCE      Energy - Oil & Gas\n",
              "1  Tata Consultancy Services         TCS  Information Technology\n",
              "2                       HDFC        HDFC      Financial Services\n",
              "3         Hindustan Unilever  HINDUNILVR          Consumer Goods\n",
              "4                    Infosys        INFY  Information Technology"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQCP7PVZMRT"
      },
      "source": [
        "df_list = df\n",
        "symbols = df_list['Symbol']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMMpK6Wx81JN",
        "outputId": "76f30e93-d761-4797-b7ec-bd8ab7c06b49"
      },
      "source": [
        "for i in range(len(symbols)):\n",
        "    ticker = symbols[i]\n",
        "    name = df_list['Company Name'][i]\n",
        "    print(ticker, name)\n",
        "    query = 'intitle:' + str(ticker)\n",
        "    query2 = 'intitle:' + str(name)\n",
        "    title = []\n",
        "    date = []\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    start_date = datetime.datetime.strptime('2011-01-01','%Y-%m-%d')\n",
        "    end_date = datetime.datetime.strptime('2021-11-01','%Y-%m-%d')\n",
        "    time_period = end_date - start_date\n",
        "    days = time_period.days\n",
        "    freq = 10\n",
        "\n",
        "    while days >= freq:\n",
        "        time_period = end_date - start_date\n",
        "        days = time_period.days\n",
        "        from_date = start_date\n",
        "        to_date = start_date + datetime.timedelta(freq)\n",
        "        if (end_date - to_date).days < 0:\n",
        "            to_date = end_date\n",
        "\n",
        "        from_date = from_date.strftime('%Y-%m-%d')\n",
        "        to_date = to_date.strftime('%Y-%m-%d')\n",
        "        start_date = start_date + datetime.timedelta(freq)\n",
        "\n",
        "        gn = GoogleNews()\n",
        "        s = gn.search(query, helper = True, when = None, from_ = from_date, to_ = to_date, proxies=None, scraping_bee=None)\n",
        "        for i in range(len(s['entries'])):\n",
        "            title.append(s['entries'][i]['title'])\n",
        "            date1 = s['entries'][i]['published_parsed']\n",
        "            date.append(time.strftime('%Y-%m-%d', date1))\n",
        "\n",
        "        gn = GoogleNews()\n",
        "        s = gn.search(query2, helper = True, when = None, from_ = from_date, to_ = to_date, proxies=None, scraping_bee=None)\n",
        "        for i in range(len(s['entries'])):\n",
        "            title.append(s['entries'][i]['title'])\n",
        "            date1 = s['entries'][i]['published_parsed']\n",
        "            date.append(time.strftime('%Y-%m-%d', date1))\n",
        "\n",
        "    df['Date'] = date\n",
        "    df['Title'] = title\n",
        "    file_name= str(ticker) + \" title.csv\"\n",
        "    df.to_csv(file_name)\n",
        "    print(query,len(df))\n",
        "    time.sleep(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RELIANCE Reliance Industries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "tkZJF25NF8mz",
        "outputId": "6520afdf-3889-4eef-c725-bd3210e94e28"
      },
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "#Get user input\n",
        "for ticker in df_list['Symbol']:\n",
        "    query = ticker\n",
        "    noOfTweet = 200000\n",
        "    tweets_list = []\n",
        "    start_date = \"2011-01-01\"\n",
        "    end_date = \"2021-11-01\"\n",
        "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query + ' lang:en since:' +  start_date + ' until:' + end_date + ' -filter:links -filter:replies').get_items()):\n",
        "        if i%1000 == 0:\n",
        "            print(i)\n",
        "        if i > int(noOfTweet):\n",
        "            break\n",
        "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n",
        "\n",
        "    #Creating a dataframe from the tweets list above \n",
        "    df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
        "    file_name=str(ticker) + \".csv\"\n",
        "    df = df[['Datetime', 'Text']]\n",
        "    df.to_csv(file_name)\n",
        "    print(query, len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4ac018a7477d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Symbol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnoOfTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snscrape'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf_TC1I1_eDC"
      },
      "source": [
        "DATE_INTERVAL_LIST=['2011-01-01 2011-07-01',\n",
        "               '2011-07-01 2012-01-01',\n",
        "               '2012-01-01 2012-07-01',\n",
        "               '2012-07-01 2013-01-01',\n",
        "               '2013-01-01 2013-07-01',\n",
        "               '2013-07-01 2014-01-01',\n",
        "               '2014-01-01 2014-07-01',\n",
        "               '2014-07-01 2015-01-01',\n",
        "               '2015-01-01 2015-07-01',\n",
        "               '2015-07-01 2016-01-01',\n",
        "               '2016-01-01 2016-07-01',\n",
        "               '2016-07-01 2017-01-01',\n",
        "               '2017-01-01 2017-07-01',\n",
        "               '2017-07-01 2018-01-01',\n",
        "               '2018-01-01 2018-07-01',\n",
        "               '2018-07-01 2019-01-01',\n",
        "               '2019-01-01 2019-07-01',\n",
        "               '2019-07-01 2020-01-01',\n",
        "               '2020-01-01 2020-07-01',\n",
        "               '2020-07-01 2021-01-01',\n",
        "               '2021-01-01 2021-07-01']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7APY2JPFsvv"
      },
      "source": [
        "KEYWORDS=df_list['Symbol']\n",
        "KEYWORDS_CODES = []\n",
        "for query in KEYWORDS:\n",
        "    # print(query)\n",
        "    KEYWORDS_CODES.append(pytrend.suggestions(keyword=query)[0])\n",
        "df_CODES= pd.DataFrame(KEYWORDS_CODES)\n",
        "df_CODES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "3pAIC9qaPH-D",
        "outputId": "cad7419e-e4fe-4bdc-9bb1-7fd64390d663"
      },
      "source": [
        "df_google_trends = pd.DataFrame()\n",
        "df_trends = pd.DataFrame()\n",
        "for DATE_INTERVAL in DATE_INTERVAL_LIST:\n",
        "  print(DATE_INTERVAL)\n",
        "  KEYWORDS_FULL=df_list['Trends key'].to_list()\n",
        "  COUNTRY=[\"US\"] #Use this link for iso country code\n",
        "  CATEGORY=0 # Use this link to select categories\n",
        "  SEARCH_TYPE='' #default is 'web searches',others include 'images','news','youtube','froogle' (google shopping)\n",
        "\n",
        "  EXACT_KEYWORDS = df_list['Trends key'].to_list()\n",
        "  Individual_EXACT_KEYWORD = list(zip(*[iter(EXACT_KEYWORDS)]*1))\n",
        "  Individual_EXACT_KEYWORD = [list(x) for x in Individual_EXACT_KEYWORD]\n",
        "  dicti = {}\n",
        "  i = 1\n",
        "  for Country in COUNTRY:\n",
        "      for keyword in Individual_EXACT_KEYWORD:\n",
        "          pytrend.build_payload(kw_list=keyword, \n",
        "                                timeframe = DATE_INTERVAL, \n",
        "                                geo = Country, \n",
        "                                cat=CATEGORY,\n",
        "                                gprop=SEARCH_TYPE) \n",
        "          dicti[i] = pytrend.interest_over_time()\n",
        "          i+=1\n",
        "  df_trends = pd.concat(dicti, axis=1)\n",
        "  df_trends.columns = df_trends.columns.droplevel(0) #drop outside header\n",
        "  df_trends = df_trends.drop('isPartial', axis = 1) #drop \"isPartial\"\n",
        "  df_trends.reset_index(level=0,inplace=True) #reset_index\n",
        "  cols = ['date']\n",
        "  for i in range(len(df_list)):\n",
        "      cols.append(df_list['Symbol'][i])\n",
        "  df_trends.columns=cols #change column names\n",
        "  print(df_trends.head())\n",
        "  df_google_trends = df_google_trends.append(df_trends)\n",
        "  print(df_google_trends.head())\n",
        "  time.sleep(1)\n",
        "\n",
        "df_google_trends"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016-01-01 2016-07-01\n",
            "        date  ASIANPAINT.BO  AXISBANK.BO  ...  TECHM.BO  TITAN.BO  ULTRACEMCO.BO\n",
            "0 2016-01-01              0           52  ...         0        22             47\n",
            "1 2016-01-02              0           32  ...         0        29             42\n",
            "2 2016-01-03              0           23  ...        20        38              0\n",
            "3 2016-01-04              0          100  ...        17        65              0\n",
            "4 2016-01-05             48           54  ...         0        41              0\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "        date  ASIANPAINT.BO  AXISBANK.BO  ...  TECHM.BO  TITAN.BO  ULTRACEMCO.BO\n",
            "0 2016-01-01              0           52  ...         0        22             47\n",
            "1 2016-01-02              0           32  ...         0        29             42\n",
            "2 2016-01-03              0           23  ...        20        38              0\n",
            "3 2016-01-04              0          100  ...        17        65              0\n",
            "4 2016-01-05             48           54  ...         0        41              0\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "2016-07-01 2017-01-01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6f98987c6622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                                 \u001b[0mgeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                 \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCATEGORY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                 gprop=SEARCH_TYPE) \n\u001b[0m\u001b[1;32m     22\u001b[0m           \u001b[0mdicti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytrend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterest_over_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytrends/request.py\u001b[0m in \u001b[0;36mbuild_payload\u001b[0;34m(self, kw_list, cat, timeframe, geo, gprop)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_payload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'req'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_payload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'req'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# get tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytrends/request.py\u001b[0m in \u001b[0;36m_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrendReq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_payload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mtrim_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )['widgets']\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# order of the json matters...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytrends/request.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             response = s.get(url, timeout=self.timeout, cookies=self.cookies,\n\u001b[0;32m--> 127\u001b[0;31m                              **kwargs, **self.requests_args)   # DO NOT USE retries or backoff_factor here\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;31m# check if the response contains json and throw an exception otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Google mostly sends 'application/json' in the Content-Type header,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caR9n4FRP7gY"
      },
      "source": [
        "df_google_trends.to_csv('Google_trends.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTr720W7K1st"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}